{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexandre-bismuth/UncertaintyInDeepLearning/blob/main/UDL_Practical_1_Setting_a_Basline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UDL Practical 1 - Setting A Baseline\n",
        "In this practical we'll go over several concepts you've likely encountered in your life as a machine learning researcher but with a different, more principled point of view. This will help you use and think about these concepts, both during the course and more broadly over the course of your research."
      ],
      "metadata": {
        "id": "pe_lrxbiad6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Typically we have some data $D$, pertaining to something in the world, and a model $M$ that we want to describe it. Here \"describe\" means effectively model it in a way that helps us make meaningful predictions about the world - is this a picture of a cat? Where should a car drive given a certain intersection? and so on and so on. We want to get the model that best describes the world, which if we're being rational, is given by $P(M|D)$.\n",
        "\n",
        "To connect it to quantities we're familiar with, using Bayes rule we have that\n",
        "$$\n",
        "P(M|D) = \\frac{P(D|M) P(M)}{P(D)}\n",
        "$$\n",
        "or\n",
        "$$\n",
        "\\text{Posterior} = \\frac{\\text{Likelihood}\\cdot\\text{Prior}}{\\text{Evidence}}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "bSmSzUkodPiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Q0:** we often talk about modelling the world yet here we have a distribution over models, $P(M|D)$, not a single one - what's our model here? Why would we want an entire distribution and not a specific one?"
      ],
      "metadata": {
        "id": "LP1jzYreeb8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A:** _answer me!_\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "N7NUE_V5euh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we have no special prior over the models, so $P(M)$ is constant, the posterior distribution simply equals the likelihood:\n",
        "$$\n",
        "P(M|D) \\propto P(D|M)\n",
        "$$\n",
        "whereas otherwise we need to take it into account:\n",
        "$$\n",
        "P(M|D) \\propto P(D|M)P(M)\n",
        "$$\n",
        "The latter can penalize extremely unlikely models by incorporating information from our prior $P(M)$."
      ],
      "metadata": {
        "id": "YhDWH-oeQkNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assuming we managed to find the distribution over models given some data - easier said than done! - we usually want to make predictions about new data $D^* = \\{(x^*, y^*)\\}$."
      ],
      "metadata": {
        "id": "F3vpvnZ_fkfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Q1:** a much easier alternative, albeit an inexact one, is finding a single model that maximizes our posterior. What is this called for each of the two cases outlined above - when our prior is uniform and when it isn't? These are common machine learning terms but you may have not encountered them yet."
      ],
      "metadata": {
        "id": "b5QrJqR4gXVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A:** _answer me!_\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "3I0QMg_IgPvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Ideally we would consider all possible models instead of just one, that way we can take our uncertainty in each into account. For example, if you're driving and see a large bus, you'd take into account both the case where a kid is about to pop out from behind it and the case when there isn't instead of immediately committing to one.\n",
        "\n",
        " In practice, this is done by weighing how well the new data is also explained by these models, giving the predictive distribution\n",
        "$$\n",
        "P(y^*|x^*, D) = \\int P(y^*|x^*, M) P(M|D) dM\n",
        "$$\n",
        "\n",
        "In practice this is difficult due to several reasons. One reason is that it's extremely hard to calculate $P(D)$ in general, as we'd have to calculate over all possible models - and so analytically calculating the predictive distribution is often impossible. More on this later!"
      ],
      "metadata": {
        "id": "YC32i60_gTXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part A - Weight Decay and Maximum A Posteriori Estimation"
      ],
      "metadata": {
        "id": "MLy-c_R232rE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When doing Maximum Likelihood Estimation (MLE), we approximate the posterior distribution with $P(M|D) \\propto P(D|M)$, and find our model weights using this distribution. Let's illustrate this using a simple linear regression example."
      ],
      "metadata": {
        "id": "jIvOm0wk_Dra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Q2:** derive from first principles the MLE solution for linear regression. To break it into steps:  \n",
        "a. Assume you have data $\\{(x_i,y_i)\\}_{i=1}^N$, where $x_i$ are vectors of features of length $d$. You can denote the $x$s stacked into a matrix of size $n\\times d$ as $X$. Assuming the data was generated using Gaussian noise with variance $\\sigma^2$, what's $P(y|x,w)$, where $w$ is the linear regression's weights? You can assume there's no bias.  \n",
        "b. What's $P(Y|X,w)$? $Y$ is similarly a vector of length $N$ of all $y_i$.  \n",
        "c. Find the log-likelihood by taking the log of (b). If we want to find $w$ what loss function are we effectively optimising?  \n",
        "d. Analytically find $w$ by taking the gradient of (c) and setting it to zero.\n",
        "\n",
        "It's okay to answer this question quickly if you've already done this, as long as you do it right :)"
      ],
      "metadata": {
        "id": "9JsIDW5djS6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A:** _answer me!_\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vJwtmhh8jS6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The case above find the MLE linear regressor. Now, let's try penalizing complex models by incorporating a prior over our weights. We typically want simpler models, which is an instance of what's known as _Occam's Razor_ - a simpler explanation is often better than a complex one. Here we can do this by assuming our weights have some prior distribution, e.g. a Gaussian with zero mean and some variance $\\tau^2$."
      ],
      "metadata": {
        "id": "ls3teGPmi4oy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Q3:** let's quickly repeat the previous derivation but now with the new prior. Again, taking it step by step:  \n",
        "a. What's the posterior distribution over the weights, given this prior?  \n",
        "b. Taking the log, what's the loss function we're optimising here?  \n",
        "c. Analytically find $w$ by taking the gradient of (b) and setting it to zero."
      ],
      "metadata": {
        "id": "KEY1HW93mDo8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A:** _answer me!_\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "XwQb1usWmDo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What you just got is known as *weight decay!* This is a common regularisation technique in machine learning in general, where we make the optimization difficult by penalizing large weights, thereby implicitly (or in the linear regression case, explicitly) preferring simpler models. Think a bit about the assumptions we made here and how changing them affects what we get. This allows us intuiting how data and modelling assumptions affect our solutions, e.g. if we have noisier data or prefer simpler models."
      ],
      "metadata": {
        "id": "9UXre8oKmAzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't need to chit-chat, let's play with this ourselves.\n",
        "\n",
        "**Q4:** please complete this linear regression implementation and see what happens when your data and prior have different variances. What sort of solutions do you get when either one is large? What would you prefer in practice?   "
      ],
      "metadata": {
        "id": "C72M8RVAoGj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generate_data(n_samples=20, input_dim=1, noise_std=0.3):\n",
        "    \"\"\"Generate synthetic data with controlled noise and outliers\"\"\"\n",
        "    true_w = torch.ones(input_dim, 1)\n",
        "\n",
        "    X = torch.randn(n_samples, input_dim)\n",
        "\n",
        "    # Regular noise\n",
        "    noise = torch.randn(n_samples, 1) * noise_std\n",
        "\n",
        "    # Add some outliers\n",
        "    n_outliers = int(0.2 * n_samples)  # 20% outliers\n",
        "    outlier_idx = torch.randperm(n_samples)[:n_outliers]\n",
        "    noise[outlier_idx] *= 10\n",
        "\n",
        "    y = X @ true_w + noise\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def plot_regression(X, y, w):\n",
        "    \"\"\"Plot data and regression line\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(X[:, 0], y, alpha=0.5, label='Data')\n",
        "\n",
        "    x_line = torch.linspace(X[:, 0].min(), X[:, 0].max(), 100).reshape(-1, 1)\n",
        "    y_line = x_line @ w.T\n",
        "\n",
        "    plt.plot(x_line, y_line, 'r-')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Generate some data\n",
        "X, y = generate_data()\n",
        "\n",
        "# Let's look at our data\n",
        "plt.scatter(X, y, alpha=0.5)\n",
        "plt.title(\"Training Data\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s8cg-Fium1iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now implement your regularized linear regression!\n",
        "# Remember to derive the solution first on paper.\n",
        "\n",
        "### fill me! ###\n",
        "# Define your noise_variance and prior_variance here\n",
        "# Implement the MAP (maximum a posteriori) solution for w\n",
        "# Hint: you'll need matrix operations\n",
        "\n",
        "# Once you have your solution, let's visualize it\n",
        "w = ### fill me! ###  # Your solution here\n",
        "plot_regression(X, y, w)\n",
        "\n",
        "# Questions to consider:\n",
        "# 1. How does your solution handle the outliers?\n",
        "# 2. Try changing your noise_variance and prior_variance - what happens?\n",
        "# 3. What happens if you make either very large or very small?\n",
        "# for your own sake, think - how would these insights translate or be implemented in more complex settings, e.g. when using a neural network?"
      ],
      "metadata": {
        "id": "PeN930VYm1pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part B - Ensembling as an Approximation of the Predictive Distribution"
      ],
      "metadata": {
        "id": "Z5Fvs1Qn3rpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although for linear regression we can compute exact solutions for all the distributions we reviewed earlier - as we did in class - typically this isn't the case. One nice but very general approximation we can use is approximating integrals by randomly sampling a few points and summing them up. For example, let's say we're computing the predictive distribution\n",
        "$$\n",
        "P(y^*|x^*, D) = \\int P(y^*|x^*, M) P(M|D) dM\n",
        "$$\n",
        "Given new data $(x^*, y^*)$ and a model $M$, we can calculate $P(y^*|x^*, M)$, but we can't (in general) precisely calculate $P(M|D)$, because we don't have $P(D)$. However, because this integral is an expectation of $P(y^*|x^*, M)$ with respect to $P(M|D)$, we can approximate it using what's known as Monte Carlo sampling. If we can sample different models in a way that's proportional to $P(M|D)$ then we can approximate the integral as a sum."
      ],
      "metadata": {
        "id": "qrsrzWJaJULx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Q5:** let's first implement the simplest example of a Monte Carlo algorithm, where we randomly sample points to calculate pi. Let's say you randomly draw 2D points in [0,1]$\\times$[0,1]. Imagine a quarter unit circle inscribed in that square, so a point falls inside it if $x^2+y^2<1$.  \n",
        "a. What's the area of that quarter circle? What's the area of that square? What's the area of the quarter circle relative to the square?  \n",
        "b. Create a short script that randomly draws many points and calculates what percent of them fall in the quarter circle. Use that and (a) to get an estimate of pi."
      ],
      "metadata": {
        "id": "Q0OW3IMRt-b0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill me!"
      ],
      "metadata": {
        "id": "lMxtqjRctOnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, back to the machine learning. In practice, if we draw models properly then we can approximate our predictive distribution as\n",
        "$$\n",
        "P(y^*|x^*, D) \\approx \\frac{1}{n}\\sum_{i=1}^n P(y^*|x^*, D, M_i)\n",
        "$$"
      ],
      "metadata": {
        "id": "LE8unw3hvEHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is often known as *ensembling*, when we use several models instead of a single one to get better results. Although quite simple, this is often used to easily get better performance from an existing setup. Historically, most models that won the ILSVRC competitions (colloquially, the ImageNet competitions, where Alexnet made its debut) used some form of ensembling.\n",
        "\n",
        "On a different note, there's still something we're hiding under the rug - how do we \"sample models\" properly? This is quite nontrivial and we'll address this properly later on. For now, we'll make do with directly optimising several models and taking them as-is."
      ],
      "metadata": {
        "id": "TfqSEvCnvfOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Q6:** optimise 3 simple networks to classify MNIST digits. How well do they perform separately vs when ensembled? Make sure you're using a GPU runtime so this'll be quick."
      ],
      "metadata": {
        "id": "rA9Z94jgx56E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Load and preprocess MNIST data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
      ],
      "metadata": {
        "id": "SRb9QS0GvETt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTNet, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(28 * 28, 512)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "cWMCqGF_tOyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "    model = MNISTNet()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(3):\n",
        "        model.train()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "\n",
        "        train_accuracy = 100. * correct / total\n",
        "        print(f'Epoch {epoch+1}/5: Train Accuracy: {train_accuracy:.2f}%')\n",
        "\n",
        "    # Test accuracy\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            output = model(data)\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    test_accuracy = 100. * correct / total\n",
        "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "_UtCiPRAvbRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model()"
      ],
      "metadata": {
        "id": "5nHCM901zlby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ensemble_predict():\n",
        "    # Train 3 models\n",
        "    print(\"Training 3 models for ensemble...\")\n",
        "    models = [train_model() for _ in range(3)]\n",
        "\n",
        "    # fill me!\n",
        "\n",
        "    print(f'Ensemble Test Accuracy: {ensemble_accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "gVA_zfXMvbCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_predict()"
      ],
      "metadata": {
        "id": "zIzbTaEVz9Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part C (short and sweet) - how should we classify?"
      ],
      "metadata": {
        "id": "qiqce6wm4H7Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Often we want to classify something as belonging to one of two categories, e.g. cats and dogs, 0 or 1, etc. Naturally, a step function is well suited for describing this:\n",
        "$$\n",
        "\\theta(x) = \\begin{cases}\n",
        "  1 & \\text{if } x \\geq 0 \\\\\n",
        "  0 & \\text{if } x < 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "However, often our inputs are nosiy. Let's say we have Gaussian noise $\\epsilon \\sim N(0, \\sigma^2)$ added to our input x. What does $\\theta(x + \\epsilon)$ look like? Can you plot it?"
      ],
      "metadata": {
        "id": "VSAM59ETsUkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Create input space\n",
        "x = np.linspace(-6, 6, 1000)\n",
        "\n",
        "# Step function (threshold at x=0)\n",
        "step = np.where(x >= 0, 1, 0)\n",
        "\n",
        "# Function to compute probability of being above 0 given Gaussian noise\n",
        "def prob_above_zero(x, noise_std):\n",
        "    ## fill me!\n",
        "    ...\n",
        "\n",
        "# Different noise levels\n",
        "noise_stds = [0.5, 1.0, 2.0, 4.0]\n",
        "\n",
        "# Create visualization\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(x, step, 'k--', label='Step Function')\n",
        "for std in noise_stds:\n",
        "    plt.plot(x, prob_above_zero(x, std), label=f'σ={std}')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Probability')\n",
        "plt.legend()\n",
        "plt.grid(True)\n"
      ],
      "metadata": {
        "id": "9bCvO1rfVGOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hmmm, this is starting to look awfully familiar. Let's see if we can leverage this intuition a bit further."
      ],
      "metadata": {
        "id": "czBACqE8UjBr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7:** let's try deriving this function analytically, specifically $\\mathbb{E}_ϵ[\\theta(x+\\epsilon)]$ where $\\epsilon\\sim N(0,\\sigma^2)$. What do we get? What would we typically call $\\sigma^2$ here?"
      ],
      "metadata": {
        "id": "WcilfTzk1wSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A:** _answer me!_"
      ],
      "metadata": {
        "id": "PzL-iJkL2aiF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8:** Often instead of this function we use a sigmoid instead. Try plotting them both and see how similar - or different - they may look given suitable parameters. Why would we prefer one over the other?"
      ],
      "metadata": {
        "id": "LCurFSXw3eml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill me!"
      ],
      "metadata": {
        "id": "Iu4hb1Gb3vOk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}